{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john-atherton/projects/text-generation/env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections \n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import unidecode\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142568, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "filename = \"../full_articles.csv\"\n",
    "filepath = os.path.join(os.getcwd(), filename)\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "data.dropna(how=\"any\", subset=[\"title\", \"content\", \"publication\"], inplace=True)\n",
    "\n",
    "SAVE_DIR = \"pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.publication.apply(lambda x: x == \"NPR\")]\n",
    "contents = data.content.tolist()\n",
    "contents = [unidecode.unidecode(content).lower() for content in contents]\n",
    "# delete the unneeded data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done tokenizing...\n"
     ]
    }
   ],
   "source": [
    "def process_content(contents_data, sequence_length=128, batch_size=128, decode_unicode=False):\n",
    "    if decode_unicode:\n",
    "        contents_data = [unidecode.unidecode(content).lower() for content in contents]\n",
    "    else:\n",
    "        contents_data = [content.lower() for content in contents]\n",
    "\n",
    "    tokenized = [[word.text for word in doc] for doc in nlp.pipe(contents_data, batch_size=128)]\n",
    "    print(\"done tokenizing...\")\n",
    "    words_dict = collections.Counter([word for doc in tokenized for word in doc])\n",
    "    words = list(sorted([word[0] for word in words_dict.most_common()]))\n",
    "    words_dict = {x: i for i,x in enumerate(words)}\n",
    "    contents_translated = [[words_dict[word] for word in doc] for doc in tokenized]\n",
    "    \n",
    "    batch_chunks = []\n",
    "    for doc in contents_translated:\n",
    "        batch_chunks_item = [doc[i:i+sequence_length+batch_size] for i in range(0, len(doc) - batch_size, batch_size)]\n",
    "        batch_chunks.extend(batch_chunks_item)\n",
    "    \n",
    "    return batch_chunks, words_dict, words\n",
    "\n",
    "content_batches, content_words_dict, content_words = process_content(contents)\n",
    "# save translators\n",
    "name = \"content_word_rnn\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"wb\") as pkl_file:\n",
    "    pickle.dump((content_batches, content_words_dict, content_words), pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"content_word_rnn\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"rb\") as pkl_file:\n",
    "    content_batches, content_words_dict, content_words = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to HDF5 file for dynamic loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "seq_length = 128\n",
    "f = h5py.File(\"rnn_data\", \"w\")\n",
    "predictor_grp = f.create_group(\"batches\")\n",
    "for i,batch in enumerate(content_batches):\n",
    "    predictor_grp.create_dataset(name=f\"batch{i}\", shape=(len(batch),), data=np.array(batch))\n",
    "f.close()\n",
    "max_i = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "class WordRNNSequence(Sequence):\n",
    "    def __init__(self, batch_idx, batch_data, seq_length, n_vocab, validation=False, training_sequence=None):\n",
    "        self.batch_idx = batch_idx\n",
    "        self.batch_data = batch_data\n",
    "        self.seq_length = seq_length\n",
    "        self.n_vocab = n_vocab\n",
    "        if validation and not training_sequence:\n",
    "            raise FileNotFoundError(\"need non-null training keras.utils.Sequence\")\n",
    "        if validation:\n",
    "            assert isinstance(training_sequence, Sequence)\n",
    "            self.training_sequence = training_sequence\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.batch_idx)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        data = self.batch_data[f\"batch{key}\"][:]\n",
    "        len_batch = data.shape[0] - self.seq_length\n",
    "        batch_x = np.zeros((len_batch, self.seq_length, self.n_vocab), dtype=np.bool)\n",
    "        for i in range(0, len_batch):\n",
    "            seq = data[i:i+self.seq_length]\n",
    "            batch_x[i,] = to_categorical(seq, num_classes=self.n_vocab)\n",
    "        batch_y = data[self.seq_length:]\n",
    "        batch_y = to_categorical(batch_y, num_classes=self.n_vocab)\n",
    "        batch_y = batch_y.astype(np.bool)\n",
    "        print(f\"{batch_x.shape}, {batch_y.shape}\")\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.validation:\n",
    "            all_batch_idx = self.batch_idx + self.training_sequence.batch_idx\n",
    "            self.training_sequence.batch_idx, self.batch_idx = train_test_split(all_batch_idx, test_size=0.2)\n",
    "               \n",
    "    def _batch_len(self, true_idx):\n",
    "        data = self.batch_data[true_idx]\n",
    "        len_batch = len(data) - int(self.seq_length)\n",
    "        return len_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def encode_sequences(sequences, word_dict, seq_length, n_vocab):\n",
    "    data = np.zeros(shape=(len(sequences), seq_length, n_vocab), dtype=np.bool) # extra word for OOV words \n",
    "    for i,sequence in enumerate(sequences): \n",
    "        if len(sequence) > seq_length:\n",
    "            sequence = sequence[:seq_length]\n",
    "        elif len(sequence) < seq_length:\n",
    "            raise NotImplementedError(f\"Need a sequence of length {seq_length}\")\n",
    "        for j,word in enumerate(sequence):\n",
    "            word_lower = word.lower()\n",
    "            if word_lower in word_dict:\n",
    "                data[i, j, word_dict[word_lower]] = 1\n",
    "            else:\n",
    "                data[i, j, n_vocab - 1] = 1\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def encode_next_words(next_words, word_dict, n_vocab):\n",
    "    next_word_encode = np.zeros(shape=(len(next_words), n_vocab), dtype=np.bool) # extra word for OOV words \n",
    "    for i,next_word in enumerate(next_words):\n",
    "        next_word_lower = next_word.lower()\n",
    "        if next_word_lower in word_dict:\n",
    "            next_word_encode[i, word_dict[next_word_lower]] = 1\n",
    "        else:\n",
    "            next_word_encode[i, n_vocab - 1] = 1\n",
    "            \n",
    "    return(next_word_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(88, 128, 97103), (88, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(18, 128, 97103), (18, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n",
      "(128, 128, 97103), (128, 97103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-14:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Bidirectional, LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(content_batches)), test_size=0.2)\n",
    "\n",
    "n_nodes = 512\n",
    "#class WordRNNSequence(Sequence):\n",
    "#    def __init__(self, batch_idx, batch_data, seq_length, n_vocab, validation=False):\n",
    "#    content_batches, content_words_dict, content_words = pickle.load(pkl_file)\n",
    "f = h5py.File(\"rnn_data\", \"r\")\n",
    "hdf5_batch_data = f[\"batches\"]\n",
    "\n",
    "train = {\n",
    "    \"batch_idx\": train_idx, \n",
    "    \"batch_data\": hdf5_batch_data, \n",
    "    \"seq_length\": 128, \n",
    "    \"n_vocab\": len(content_words)\n",
    "}\n",
    "\n",
    "valid = {\n",
    "    \"batch_idx\": validation_idx, \n",
    "    \"batch_data\": hdf5_batch_data, \n",
    "    \"seq_length\": 128, \n",
    "    \"n_vocab\": len(content_words)\n",
    "}\n",
    "\n",
    "train_sequence = WordRNNSequence(**train)\n",
    "valid_sequence = WordRNNSequence(training_sequence=train_sequence, **valid)\n",
    "\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "N_VOCAB = len(content_words)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_nodes, activation=\"relu\"), input_shape = (SEQ_LENGTH, N_VOCAB)))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Dense(N_VOCAB))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "callbacks = [EarlyStopping(patience=2, monitor=\"val_loss\")]\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "MODEL_CHECK_DIR = \"checkpoints\"\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath='model_gen.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0, mode='auto',\n",
    "                           period=2)]\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model.fit_generator(train_sequence,\n",
    "                    epochs = epochs,\n",
    "                    validation_data=valid_sequence,\n",
    "                    callbacks=callbacks,\n",
    "                    use_multiprocessing=True,\n",
    "                    max_queue_size=5)\n",
    "\n",
    "\n",
    "model.save(os.path.join(os.getcwd(), MODEL_CHECK_DIR, 'model_gen_title.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"rnn_data\" (mode r)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    t = np.asarray(preds).astype(np.float64)\n",
    "    t = np.log(t) / temperature\n",
    "    t = np.exp(t)\n",
    "    t = t / np.sum(t)\n",
    "    probs = np.random.multinomial(1, t, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def gen_words(model, seed, word_num, word_dict, seq_length, words_index, temperature=1.0):\n",
    "    \"\"\"`nlp` must be defined\"\"\"\n",
    "    words = [word.text for word in nlp(seed) if word.is_alpha or word.is_punct]\n",
    "    \n",
    "    generated = words\n",
    "\n",
    "\n",
    "    for i in range(word_num):\n",
    "        encoded = encode_sequence(words, word_dict, seq_length)\n",
    "        preds = model.predict(encoded)[0]\n",
    "        result = sample(preds, temperature)\n",
    "        next_word = words_index[result]\n",
    "        generated.append(next_word)\n",
    "        words = words[1:] + [next_word]\n",
    "        \n",
    "    return \" \".join(generated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'breaking news : trump announced a speech in which he denounced has called seen been to in to his his the life life presidency because , , a many there notes likely never many on a small more baby group than than , the any right tv professor , . , ” ” but i i it know have , in to i a my like very mind album , , . i i you think am know that is that the the this way national administration is company . often in has only all a creating over familiar ways all , the the ” national small and war source people in of to the the make new community line business , . at but but least all there years of are , trump more though in than , another a putin parents year more would . to take ” remember the a any ” museum genome video director . . . ” but .. they they the were say same , in plan ” which that to is happens be still to the . repeal move ” of in : that obamacare you people and can would no find follow follow why her , would of pontzer have things to'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_words(model=model, seed=\"breaking news: trump announced a speech in which he denounced\",\n",
    "          word_num=200,\n",
    "          word_dict=content_word_dict,\n",
    "          seq_length=10,\n",
    "          words_index=content_vocab,\n",
    "         temperature=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
