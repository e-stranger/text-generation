{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections \n",
    "import spacy\n",
    "import pickle\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "filename = \"full_articles.csv\"\n",
    "filepath = os.path.join(os.getcwd(), filename)\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "data.dropna(how=\"any\", subset=[\"title\", \"content\", \"publication\"], inplace=True)\n",
    "\n",
    "SAVE_DIR = \"pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.publication.apply(lambda x: x == \"NPR\")]\n",
    "contents = data.content.tolist()\n",
    "contents = [content.lower() for content in contents]\n",
    "# delete the unneeded data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and tokenize\n",
    "content_words = [word.text for doc in nlp.pipe(contents, batch_size=128) for word in doc if word.is_alpha or word.is_punct]\n",
    "# create translator\n",
    "content_count = collections.Counter(content_words)\n",
    "del content_words\n",
    "# create index-to-word translator\n",
    "content_vocab = list(sorted([item[0] for item in content_count.most_common()]))\n",
    "# create word-to-index translator \n",
    "content_word_dict = {x: i for i,x in enumerate(content_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save translators\n",
    "name = \"content\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"wb\") as pkl_file:\n",
    "    pickle.dump((contents, content_vocab, content_word_dict), pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load translators\n",
    "name = \"content\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"rb\") as pkl_file:\n",
    "    contents, content_vocab, content_word_dict = pickle.load(pkl_file)\n",
    "    \n",
    "#name = \"title\"\n",
    "#PKL_SAVE = f\"{name}.pkl\"\n",
    "#with open(os.path.join(\"pickles\",PKL_SAVE), \"rb\") as pkl_file:\n",
    "#    title_words, title_vocab, title_word_dict = pickle.load(pkl_file)\n",
    "    \n",
    "#title_word_dict = {x: i for i,x in enumerate(title_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11992"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(sequences, word_dict, seq_length, n_vocab):\n",
    "    data = np.zeros(shape=(len(sequences), seq_length, n_vocab), dtype=np.bool)\n",
    "    for sequence in sequences: \n",
    "        if len(sequence) > seq_length:\n",
    "            sequence = sequence[:seq_length]\n",
    "        elif len(sequence) < seq_length:\n",
    "            raise NotImplementedError(f\"Need a sequence of length {seq_length}\")\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j,word in enumerate(sequence):\n",
    "            data[i, j, word_dict[word.lower()]] = 1\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def encode_next_words(next_words, word_dict, n_vocab):\n",
    "    next_word_encode = np.zeros(shape=(len(next_words), n_vocab), dtype=np.bool)\n",
    "    for i,next_word in enumerate(next_words):\n",
    "        next_word_encode[i, word_dict[next_word]] = 1\n",
    "    return(next_word_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = contents\n",
    "COUNTER = 0\n",
    "VALIDATION_SPLIT = 0.1\n",
    "VALIDATION_INDEX = math.floor((1-VALIDATION_SPLIT) * len(DATA))\n",
    "PUBLICATION = \"NPR\"\n",
    "def fit_gen(word_dict, seq_length, n_vocab, step_data=100, step_token=1, validation=False):\n",
    "    print(f\"fit gen called for {seq_length} length sequences\")\n",
    "    global DATA, TRAINING_DATA, VALIDATION_DATA, VALIDATION_INDEX, COUNTER\n",
    "    if validation:\n",
    "        \n",
    "        num_seq = len(VALIDATION_DATA)\n",
    "    else:\n",
    "        np.random.shuffle(DATA)\n",
    "        TRAINING_DATA = DATA[:VALIDATION_INDEX]\n",
    "        VALIDATION_DATA = DATA[VALIDATION_INDEX:]\n",
    "        num_seq = len(TRAINING_DATA)\n",
    "    total_iterations = math.floor((num_seq - step_data) // step_data)\n",
    "    while True:\n",
    "        for i in range(0, total_iterations, step_data):\n",
    "            print(f\"iteration {i} out of {total_iterations}\")\n",
    "            if validation:\n",
    "                contents = VALIDATION_DATA[i:i+step_data]\n",
    "            else:\n",
    "                contents = TRAINING_DATA[i:i+step_data]           \n",
    "            sequences = []\n",
    "            next_words = []\n",
    "            print(\"here1\")\n",
    "            content_words = [word.text for doc in nlp.pipe(contents, batch_size=step_data) for word in doc if word.is_alpha or word.is_punct]\n",
    "            for j in range(0, len(content_words)-seq_length, step_token):\n",
    "                sequence = content_words[j:j+seq_length]\n",
    "                next_word = content_words[j+seq_length]\n",
    "                sequences.append(sequence)\n",
    "                next_words.append(next_word)\n",
    "            del content_words\n",
    "            print(\"here2\")\n",
    "            training = encode_sequences(sequences, word_dict, seq_length, n_vocab)\n",
    "            targets =  encode_next_words(next_words, word_dict, n_vocab)\n",
    "            training_data = (training, targets)\n",
    "            np.save(os.path.join(os.getcwd(), f\"data/training/training{COUNTER}.npy\"), training)\n",
    "            print(f\"SAVED training{COUNTER}.npy\")\n",
    "            np.save(os.path.join(os.getcwd(), f\"data/targets/targets{COUNTER}.npy\"), targets)\n",
    "            print(f\"SAVED targets{COUNTER}.npy\")\n",
    "\n",
    "            COUNTER += 1\n",
    "\n",
    "            yield training_data\n",
    "            del next_words, sequences, content_words, training, targets\n",
    "        exit(0)\n",
    "\n",
    "        #yield training_data\n",
    "import concurrent.futures\n",
    "\n",
    "def fit_gen2(word_dict, seq_length, n_vocab, step_data=100, step_token=1):\n",
    "    print(f\"fit gen called for {seq_length} length sequences\")\n",
    "    global DATA, TRAINING_DATA, VALIDATION_DATA, VALIDATION_INDEX, COUNTER\n",
    "\n",
    "    np.random.shuffle(DATA)\n",
    "    TRAINING_DATA = DATA\n",
    "        #TRAINING_DATA = DATA[:VALIDATION_INDEX]\n",
    "        #VALIDATION_DATA = DATA[VALIDATION_INDEX:]\n",
    "    num_seq = len(TRAINING_DATA)\n",
    "    total_iterations = math.floor((num_seq - step_data) // step_data)\n",
    "    \n",
    "    def process_i(index):\n",
    "        temp_ct = int(index / step_data)\n",
    "        i = index\n",
    "        print(f\"iteration {temp_ct} out of {total_iterations}\")\n",
    "\n",
    "        contents = TRAINING_DATA[i:i+step_data]           \n",
    "        sequences = []\n",
    "        next_words = []\n",
    "        content_words = [word.text for doc in nlp.pipe(contents, batch_size=step_data) for word in doc if word.is_alpha or word.is_punct]\n",
    "        for j in range(0, len(content_words)-seq_length, step_token):\n",
    "            sequence = content_words[j:j+seq_length]\n",
    "            next_word = content_words[j+seq_length]\n",
    "            sequences.append(sequence)\n",
    "            next_words.append(next_word)\n",
    "        del content_words\n",
    "        training = encode_sequences(sequences, word_dict, seq_length, n_vocab)\n",
    "        targets =  encode_next_words(next_words, word_dict, n_vocab)\n",
    "        training_data = (training, targets)\n",
    "        np.save(os.path.join(os.getcwd(), f\"data/training/training_{PUBLICATION}_{temp_ct}.npy\"), training)\n",
    "        print(f\"SAVED training{temp_ct}.npy\")\n",
    "        np.save(os.path.join(os.getcwd(), f\"data/target/target_{PUBLICATION}_{temp_ct}.npy\"), targets)\n",
    "        print(f\"SAVED targets{temp_ct}.npy\")\n",
    "        del next_words, sequences, content_words, training, targets\n",
    "        return i\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        executor.map(process_i, range(0, total_iterations, step_data))\n",
    "        #yield training_data\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_samples = len(contents)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit gen called for 15 length sequences\n",
      "iteration 0 out of 748iteration 1 out of 748\n",
      "\n",
      "iteration 2 out of 748iteration 3 out of 748\n",
      "iteration 4 out of 748\n",
      "\n",
      "iteration 5 out of 748iteration 6 out of 748\n",
      "iteration 7 out of 748\n",
      "\n",
      "SAVED training5.npy\n",
      "SAVED targets5.npy\n",
      "iteration 8 out of 748\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 15\n",
    "batch_size = 16\n",
    "fit_gen2(content_word_dict, SEQ_LENGTH, n_vocab=len(content_word_dict), step_data=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit gen called for 10 length sequences\n",
      "fit gen called for 10 length sequencesEpoch 1/15\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [93389,2048] and type float\n\t [[{{node training_2/Adam/zeros}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [93389,2048] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training_2/Adam/zeros', defined at:\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-03b2767e55da>\", line 40, in <module>\n    validation_steps=validation_steps)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\", line 1415, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 39, in fit_generator\n    model._make_train_function()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\", line 498, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/optimizers.py\", line 482, in get_updates\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/optimizers.py\", line 482, in <listcomp>\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 700, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1548, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2817, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1750, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [93389,2048] and type float\n\t [[{{node training_2/Adam/zeros}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [93389,2048] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [93389,2048] and type float\n\t [[{{node training_2/Adam/zeros}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [93389,2048] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-03b2767e55da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_generator_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                    validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [93389,2048] and type float\n\t [[{{node training_2/Adam/zeros}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [93389,2048] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training_2/Adam/zeros', defined at:\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-03b2767e55da>\", line 40, in <module>\n    validation_steps=validation_steps)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\", line 1415, in fit_generator\n    initial_epoch=initial_epoch)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training_generator.py\", line 39, in fit_generator\n    model._make_train_function()\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\", line 498, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/optimizers.py\", line 482, in get_updates\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/optimizers.py\", line 482, in <listcomp>\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 700, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1548, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2817, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3254, in create_op\n    op_def=op_def)\n  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1750, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [93389,2048] and type float\n\t [[{{node training_2/Adam/zeros}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [93389,2048] values: [0 0 0...]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Bidirectional, LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "n_nodes = 512\n",
    "n_vocab = len(content_word_dict)\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_nodes, activation=\"relu\"), input_shape = (SEQ_LENGTH, n_vocab)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "callbacks = [EarlyStopping(patience=2, monitor=\"val_loss\")]\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "MODEL_CHECK_DIR = \"checkpoints\"\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=os.path.join(os.getcwd(), \n",
    "                                                 MODEL_CHECK_DIR,\n",
    "                                                 'model_gen.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0, mode='auto',\n",
    "                           period=2)]\n",
    "\n",
    "batch_size = 12\n",
    "epochs = 15\n",
    "steps_per_epoch = int(len(DATA) * (1-VALIDATION_SPLIT) // batch_size)\n",
    "validation_steps = int(len(DATA) * VALIDATION_SPLIT // batch_size)\n",
    "\n",
    "fit_generator = lambda : fit_gen(content_word_dict, SEQ_LENGTH, n_vocab=len(content_word_dict), step_data=batch_size)\n",
    "fit_generator_validation = lambda : fit_gen(content_word_dict, SEQ_LENGTH, n_vocab=len(content_word_dict), validation=True, step_data=batch_size)\n",
    "\n",
    "model.fit_generator(fit_generator(),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs = epochs,\n",
    "                    validation_data=fit_generator_validation(),\n",
    "                   validation_steps=validation_steps)\n",
    "\n",
    "\n",
    "model.save(os.path.join(os.getcwd(), MODEL_CHECK_DIR, 'model_gen_title.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    t = np.asarray(preds).astype(np.float64)\n",
    "    t = np.log(t) / temperature\n",
    "    t = np.exp(t)\n",
    "    t = t / np.sum(t)\n",
    "    probs = np.random.multinomial(1, t, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def gen_words(model, seed, word_num, word_dict, seq_length, words_index, temperature=1.0):\n",
    "    \"\"\"`nlp` must be defined\"\"\"\n",
    "    words = [word.text for word in nlp(seed) if word.is_alpha or word.is_punct]\n",
    "    \n",
    "    generated = words\n",
    "\n",
    "\n",
    "    for i in range(word_num):\n",
    "        encoded = encode_sequence(words, word_dict, seq_length)\n",
    "        preds = model.predict(encoded)[0]\n",
    "        result = sample(preds, temperature)\n",
    "        next_word = words_index[result]\n",
    "        generated.append(next_word)\n",
    "        words = words[1:] + [next_word]\n",
    "        \n",
    "    return \" \".join(generated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'breaking news : trump announced a speech in which he denounced has called seen been to in to his his the life life presidency because , , a many there notes likely never many on a small more baby group than than , the any right tv professor , . , ” ” but i i it know have , in to i a my like very mind album , , . i i you think am know that is that the the this way national administration is company . often in has only all a creating over familiar ways all , the the ” national small and war source people in of to the the make new community line business , . at but but least all there years of are , trump more though in than , another a putin parents year more would . to take ” remember the a any ” museum genome video director . . . ” but .. they they the were say same , in plan ” which that to is happens be still to the . repeal move ” of in : that obamacare you people and can would no find follow follow why her , would of pontzer have things to'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_words(model=model, seed=\"breaking news: trump announced a speech in which he denounced\",\n",
    "          word_num=200,\n",
    "          word_dict=content_word_dict,\n",
    "          seq_length=10,\n",
    "          words_index=content_vocab,\n",
    "         temperature=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
