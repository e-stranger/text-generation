{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections \n",
    "import spacy\n",
    "import pickle\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import unidecode\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "filename = \"../full_articles.csv\"\n",
    "filepath = os.path.join(os.getcwd(), filename)\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "data.dropna(how=\"any\", subset=[\"title\", \"content\", \"publication\"], inplace=True)\n",
    "\n",
    "SAVE_DIR = \"pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data.publication.apply(lambda x: x == \"NPR\")]\n",
    "contents = data.content.tolist()\n",
    "contents = [unidecode.unidecode(content).lower() for content in contents]\n",
    "# delete the unneeded data\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(sequences, word_dict, seq_length, n_vocab):\n",
    "    data = np.zeros(shape=(len(sequences), seq_length, n_vocab), dtype=np.bool)\n",
    "    for i,sequence in enumerate(sequences): \n",
    "        if len(sequence) > seq_length:\n",
    "            sequence = sequence[:seq_length]\n",
    "        elif len(sequence) < seq_length:\n",
    "            raise NotImplementedError(f\"Need a sequence of length {seq_length}\")\n",
    "        for j,word in enumerate(sequence):\n",
    "            data[i, j, word_dict[word.lower()]] = 1\n",
    "    return(data)\n",
    "\n",
    "def encode_next_words(next_words, word_dict, n_vocab):\n",
    "    next_word_encode = np.zeros(shape=(len(next_words), n_vocab), dtype=np.bool)\n",
    "    for i,next_word in enumerate(next_words):\n",
    "        next_word_encode[i, word_dict[next_word]] = 1\n",
    "    return(next_word_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gen(word_dict,\n",
    "            seq_length, \n",
    "            n_vocab, \n",
    "            step_data=100, \n",
    "            step_token=1, \n",
    "            validation=False, \n",
    "            char_level=False):\n",
    "    print(f\"fit gen called for {seq_length} length sequences\")\n",
    "    global DATA, TRAINING_DATA, VALIDATION_DATA, VALIDATION_INDEX\n",
    "    if validation:\n",
    "        \n",
    "        num_seq = len(VALIDATION_DATA)\n",
    "    else:\n",
    "        np.random.shuffle(DATA)\n",
    "        TRAINING_DATA = DATA[:VALIDATION_INDEX]\n",
    "        VALIDATION_DATA = DATA[VALIDATION_INDEX:]\n",
    "        num_seq = len(TRAINING_DATA)\n",
    "    total_iterations = math.floor((num_seq - step_data) // step_data)\n",
    "    while True:\n",
    "        for i in range(0, num_seq - step_data, step_data):\n",
    "            n_it = i / step_data\n",
    "            print(f\"iteration {n_it} out of {total_iterations}\")\n",
    "            if validation:\n",
    "                contents = VALIDATION_DATA[i:i+step_data]\n",
    "            else:\n",
    "                contents = TRAINING_DATA[i:i+step_data]           \n",
    "            sequences = []\n",
    "            next_words = []\n",
    "            if char_level:\n",
    "                content_words = [word for doc in TRAINING_DATA for word in doc]\n",
    "            else:\n",
    "                content_words = [word.text for doc in nlp.pipe(contents, batch_size=step_data) for word in doc if word.is_alpha or word.is_punct]\n",
    "            for j in range(0, len(content_words)-seq_length, step_token):\n",
    "                sequence = content_words[j:j+seq_length]\n",
    "                next_word = content_words[j+seq_length]\n",
    "                sequences.append(sequence)\n",
    "                next_words.append(next_word)\n",
    "            del content_words\n",
    "\n",
    "            training = encode_sequences(sequences, word_dict, seq_length, n_vocab)\n",
    "            target =  encode_next_words(next_words, word_dict, n_vocab)\n",
    "            assert training.shape[0] == target.shape[0]\n",
    "            training_data = (training, target)\n",
    "            \n",
    "            np.save(os.path.join(os.getcwd(), f\"data/training/training_{PUBLICATION}_{step_data}_{n_it}.npy\"), training)\n",
    "            print(f\"SAVED training{n_it}.npy\")\n",
    "            np.save(os.path.join(os.getcwd(), f\"data/target/target_{PUBLICATION}_{step_data}_{n_it}.npy\"), target)\n",
    "            print(f\"SAVED target{n_it}.npy\")\n",
    "\n",
    "            yield training_data\n",
    "            del next_words, sequences, content_words, training, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_chars = [c for content in contents for c in content]\n",
    "content_char_count = collections.Counter(content_chars)\n",
    "content_char_ind = [char[0] for char in content_char_count.most_common()]\n",
    "content_char_dict = {char: i for i, char in enumerate(content_char_ind)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def fit_gen(word_dict, seq_length, n_vocab, step_data=100, step_token=1, validation=False):\n",
    "batch_size = 64\n",
    "seq_length\n",
    "fit_gen_char = lambda : fit_gen(content_char_dict, seq_length=seq_length, n_vocab=len(content_char_dict), step_data=batch_size, char_level=True)\n",
    "fit_gen_char_val = lambda : fit_gen(content_char_dict, seq_length=seq_length, n_vocab=len(content_char_dict), step_data=batch_sze, validation=True, char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save translators\n",
    "name = \"content_char\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"wb\") as pkl_file:\n",
    "    pickle.dump((content_chars, content_ind, content_dict), pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = contents\n",
    "VALIDATION_SPLIT = 0.1\n",
    "VALIDATION_INDEX = math.floor((1-VALIDATION_SPLIT) * len(DATA))\n",
    "PUBLICATION = \"NPR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and tokenize\n",
    "#content_words = [word.text for doc in nlp.pipe(contents, batch_size=128) for word in doc if word.is_alpha or word.is_punct]\n",
    "# create translator\n",
    "#content_count = collections.Counter(content_words)\n",
    "#del content_words\n",
    "# create index-to-word translator\n",
    "#content_vocab = list(sorted([item[0] for item in content_count.most_common()]))\n",
    "# create word-to-index translator \n",
    "#content_word_dict = {x: i for i,x in enumerate(content_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save translators\n",
    "#name = \"content\"\n",
    "#PKL_SAVE = f\"{name}.pkl\"\n",
    "#with open(os.path.join(\"pickles\",PKL_SAVE), \"wb\") as pkl_file:\n",
    "#    pickle.dump((contents, content_vocab, content_word_dict), pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load translators\n",
    "#name = \"content\"\n",
    "#PKL_SAVE = f\"{name}.pkl\"\n",
    "#with open(os.path.join(\"pickles\",PKL_SAVE), \"rb\") as pkl_file:\n",
    "#    contents, content_vocab, content_word_dict = pickle.load(pkl_file)\n",
    "    \n",
    "#name = \"title\"\n",
    "#PKL_SAVE = f\"{name}.pkl\"\n",
    "#with open(os.path.join(\"pickles\",PKL_SAVE), \"rb\") as pkl_file:\n",
    "#    title_words, title_vocab, title_word_dict = pickle.load(pkl_file)\n",
    "    \n",
    "#title_word_dict = {x: i for i,x in enumerate(title_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/john-atherton/projects/text-generation/env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "fit gen called for 100 length sequencesfit gen called for 100 length sequences\n",
      "\n",
      "iteration 0.0 out of 673\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Bidirectional, LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "n_nodes = 512\n",
    "SEQ_LENGTH = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_nodes, activation=\"relu\"), input_shape = (SEQ_LENGTH, n_vocab)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "callbacks = [EarlyStopping(patience=2, monitor=\"val_loss\")]\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "MODEL_CHECK_DIR = \"checkpoints\"\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=os.path.join(os.getcwd(), \n",
    "                                                 MODEL_CHECK_DIR,\n",
    "                                                 'model_gen.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0, mode='auto',\n",
    "                           period=2)]\n",
    "\n",
    "batch_size = 12\n",
    "epochs = 15\n",
    "steps_per_epoch = int(len(DATA) * (1-VALIDATION_SPLIT) // batch_size)\n",
    "validation_steps = int(len(DATA) * VALIDATION_SPLIT // batch_size)\n",
    "\n",
    "fit_generator = lambda : fit_gen(content_word_dict, SEQ_LENGTH, n_vocab=len(content_word_dict), step_data=batch_size)\n",
    "fit_generator_validation = lambda : fit_gen(content_word_dict, SEQ_LENGTH, n_vocab=len(content_word_dict), validation=True, step_data=batch_size)\n",
    "\n",
    "model.fit_generator(f(),\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs = epochs,\n",
    "                    validation_data=g(),\n",
    "                   validation_steps=validation_steps)\n",
    "\n",
    "\n",
    "model.save(os.path.join(os.getcwd(), MODEL_CHECK_DIR, 'model_gen_title.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    t = np.asarray(preds).astype(np.float64)\n",
    "    t = np.log(t) / temperature\n",
    "    t = np.exp(t)\n",
    "    t = t / np.sum(t)\n",
    "    probs = np.random.multinomial(1, t, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def gen_words(model, seed, word_num, word_dict, seq_length, words_index, temperature=1.0):\n",
    "    \"\"\"`nlp` must be defined\"\"\"\n",
    "    words = [word.text for word in nlp(seed) if word.is_alpha or word.is_punct]\n",
    "    \n",
    "    generated = words\n",
    "\n",
    "\n",
    "    for i in range(word_num):\n",
    "        encoded = encode_sequence(words, word_dict, seq_length)\n",
    "        preds = model.predict(encoded)[0]\n",
    "        result = sample(preds, temperature)\n",
    "        next_word = words_index[result]\n",
    "        generated.append(next_word)\n",
    "        words = words[1:] + [next_word]\n",
    "        \n",
    "    return \" \".join(generated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'breaking news : trump announced a speech in which he denounced has called seen been to in to his his the life life presidency because , , a many there notes likely never many on a small more baby group than than , the any right tv professor , . , ” ” but i i it know have , in to i a my like very mind album , , . i i you think am know that is that the the this way national administration is company . often in has only all a creating over familiar ways all , the the ” national small and war source people in of to the the make new community line business , . at but but least all there years of are , trump more though in than , another a putin parents year more would . to take ” remember the a any ” museum genome video director . . . ” but .. they they the were say same , in plan ” which that to is happens be still to the . repeal move ” of in : that obamacare you people and can would no find follow follow why her , would of pontzer have things to'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_words(model=model, seed=\"breaking news: trump announced a speech in which he denounced\",\n",
    "          word_num=200,\n",
    "          word_dict=content_word_dict,\n",
    "          seq_length=10,\n",
    "          words_index=content_vocab,\n",
    "         temperature=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
