{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections \n",
    "import spacy\n",
    "import pickle\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "\n",
    "\n",
    "filename = \"full_articles.csv\"\n",
    "filepath = os.path.join(os.getcwd(), filename)\n",
    "data = pd.read_csv(filepath)\n",
    "\n",
    "data.dropna(how=\"any\", subset=[\"title\", \"content\", \"publication\"], inplace=True)\n",
    "\n",
    "SEQ_LENGTH = 30\n",
    "STEP = 1\n",
    "SAVE_DIR = \"pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = data.content[:100].tolist()\n",
    "contents = data.content[:100].tolist() # change to all later\n",
    "\n",
    "\n",
    "titles = [title.lower() for title in titles] # change to all later\n",
    "contents = [content.lower() for content in contents ] # change to all later\n",
    "\n",
    "content_words = [word.text for doc in tokenizer.pipe(contents, batch_size=128) for word in doc]\n",
    "title_words = [word.text for doc in tokenizer.pipe(titles, batch_size=128) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_count = collections.Counter(content_words)\n",
    "title_count = collections.Counter(title_words)\n",
    "\n",
    "content_vocab = list(sorted([item[0] for item in content_count.most_common()]))\n",
    "title_vocab = list(sorted([item[0] for item in title_count.most_common()]))\n",
    "\n",
    "content_word_dict = [{x: i} for i,x in enumerate(content_vocab)]\n",
    "title_word_dict = [{x: i} for i,x in enumerate(title_vocab)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"content\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"wb\") as pkl_file:\n",
    "    pickle.dump((content_words, content_vocab, content_word_dict), pkl_file)\n",
    "    \n",
    "name = \"title\"\n",
    "PKL_SAVE = f\"{name}.pkl\"\n",
    "with open(os.path.join(\"pickles\",PKL_SAVE), \"wb\") as pkl_file:\n",
    "    pickle.dump((content_words, content_vocab, content_word_dict), pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 30\n",
    "STEP_SIZE = 1\n",
    "\n",
    "X_raw_content = []    \n",
    "y_raw_content = []\n",
    "\n",
    "for i in range(0, len(content_words) - SEQ_LENGTH, STEP_SIZE):\n",
    "    sequence = content_words[i:i+SEQ_LENGTH]\n",
    "    next_word = content_words[i+SEQ_LENGTH]\n",
    "    X_raw_content.append(sequence)\n",
    "    y_raw_content.append(next_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY DOING content RN\n",
    "# X = (num_sequences, SEQ_LENGTH, len(vocab))\n",
    "# Y = (num_sequences, len_vocab)\n",
    "n_sequences = len(X_raw_content)\n",
    "n_vocab = len(content_vocab)\n",
    "X_content = np.zeros(shape=(n_sequences, SEQ_LENGTH, n_vocab))\n",
    "y_content = np.zeros(shape=(n_sequences, n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Bidirectional, LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "n_nodes = 258\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_nodes, activation=\"relu\"), input_shape = (SEQ_LENGTH, n_vocab)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "callbacks = [EarlyStopping(patience=2, monitor=\"val_loss\")]\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "MODEL_CHECK_DIR = \"/checkpoints\"\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=MODEL_CHECK_DIR + \"/\" + 'model_gen.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "model.fit(X_content, y_content,\n",
    "             batch_size = batch_size,\n",
    "             epochs=epochs,\n",
    "             shuffle=True,\n",
    "              validation_split=0.1\n",
    "             )\n",
    "\n",
    "md.save(MODEL_CHECK_DIR + \"/\" + 'model_gen.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
